{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GBZpEenkTeu"
      },
      "source": [
        "---\n",
        "## 💾 Drive\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5ZrHBp4a3vZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEVXqhT6cIpY"
      },
      "outputs": [],
      "source": [
        "BASE_FOLDER = '/content/drive/MyDrive/TFM/'\n",
        "# RACE\n",
        "RACE_DIR = '/content/drive/MyDrive/TFM/RACE_DATASET/output_dir/RACE'\n",
        "# EE\n",
        "EE_EN_PATH = BASE_FOLDER + 'EntranceExam/qa2015-exam-readingENGLISH.csv'\n",
        "EE_ES_PATH = BASE_FOLDER + 'EntranceExam/qa2015-exam-readingSPANISH.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 🧹 Clean Caches\n",
        "---"
      ],
      "metadata": {
        "id": "Cvqi8105rFuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check GPU & RAM"
      ],
      "metadata": {
        "id": "H_FI8ftjcrWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "JIfJ-pmYcpOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flush Memory"
      ],
      "metadata": {
        "id": "GsfZCmfIKJkl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj-ULEruy6WD"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.cache/huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache() "
      ],
      "metadata": {
        "id": "PoFgB7ue8J91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 🔥 Pytorch Lightning\n",
        "---"
      ],
      "metadata": {
        "id": "eM5Bm_md_b51"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE_2lDGY2eSx"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL2DGIQHJTF-"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SqfExlcu-FK"
      },
      "outputs": [],
      "source": [
        "!pip install lineflow transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2641c2M2fKd"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "from pathlib import Path\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "import lineflow as lf\n",
        "import transformers\n",
        "from transformers import DistilBertForMultipleChoice, DistilBertTokenizerFast, AdamW\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLN0aIi7kV62"
      },
      "source": [
        "## RACE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLU9QeD2qgoE"
      },
      "source": [
        "### Download models etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmZN853zjw44"
      },
      "source": [
        "#### Download Original File\n",
        "https://www.cs.cmu.edu/~glai1/data/race/\n",
        "\n",
        "\n",
        "### Download from datasets library\n",
        "> from datasets import load_dataset\n",
        "\n",
        "> race_all = load_dataset(\"race\", \"all\")\n",
        "\n",
        "### Extract from drive to colab root \n",
        "!tar -xvf /content/drive/MyDrive/TFM/RACE_DATASET/RACE.tar.gz \n",
        "\n",
        "### Extract from drive to drive directory\n",
        "\n",
        "!tar -xvf /content/drive/MyDrive/TFM/RACE_DATASET/RACE.tar.gz -C /content/drive/MyDrive/TFM/RACE_DATASET/output_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHdDaOikgFnr"
      },
      "outputs": [],
      "source": [
        "#!tar -xvf /content/drive/MyDrive/TFM/RACE_DATASET/RACE.tar.gz -C ./RACE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB_1nx-eN-Fi"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkxMFwIgNupV"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"distilbert-base-uncased\"\n",
        "MODEL_TRF = DistilBertForMultipleChoice\n",
        "MODEL_TKN = DistilBertTokenizerFast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyl6qzJjb0qi"
      },
      "source": [
        "### Race loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn99_VhYyzXF"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 10000\n",
        "NUM_LABELS = 4\n",
        "label_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
        "# RACE \n",
        "RACE_DIR = '/content/drive/MyDrive/TFM/RACE_DATASET/output_dir/RACE'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9ILd9Q4bsPF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from os.path import exists\n",
        "\n",
        "def raw_samples_to_dataset(samples, size=100):\n",
        "    datas = []\n",
        "    for sample in samples:\n",
        "        for idx in range(len(sample[\"answers\"])):\n",
        "            _id = sample[\"id\"] if \"id\" in sample else sample[\"example_id\"]\n",
        "            _article = sample[\"article\"]\n",
        "            _answer = sample[\"answers\"][idx]\n",
        "            _options = sample[\"options\"][idx]\n",
        "            _question = sample[\"questions\"][idx]\n",
        "\n",
        "            data = {\n",
        "              \"id\": _id,\n",
        "              \"article\": _article,\n",
        "              \"answer\": _answer,\n",
        "              \"options\": _options,\n",
        "              \"question\": _question,\n",
        "            }\n",
        "            datas.append(data)\n",
        "    return lf.Dataset(datas)\n",
        "\n",
        "\n",
        "def preprocess(tokenizer: MODEL_TKN, x: Dict) -> Dict:\n",
        "\n",
        "    choices_features = []\n",
        "\n",
        "    option: str\n",
        "    for option in x[\"options\"]:\n",
        "        text_a = x[\"article\"]\n",
        "        if x[\"question\"].find(\"_\") != -1:\n",
        "            text_b = x[\"question\"].replace(\"_\", option)\n",
        "        else:\n",
        "            text_b = x[\"question\"] + \" \" + option\n",
        "\n",
        "        inputs = tokenizer.encode_plus(\n",
        "                text_a,\n",
        "                text_b,\n",
        "                add_special_tokens=True,\n",
        "                max_length=MAX_LEN\n",
        "                )\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        pad_token_id = tokenizer.pad_token_id\n",
        "        padding_length = MAX_LEN - len(input_ids)\n",
        "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
        "        attention_mask = attention_mask + ([0] * padding_length)\n",
        "\n",
        "        assert len(input_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(input_ids), MAX_LEN)\n",
        "        assert len(attention_mask) == MAX_LEN, \"Error with input length {} vs {}\".format(len(attention_mask), MAX_LEN)\n",
        "\n",
        "        choices_features.append({\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask\n",
        "        })\n",
        "\n",
        "    labels = label_map.get(x[\"answer\"], -1)\n",
        "    label = torch.tensor(labels).long()\n",
        "\n",
        "    return {\n",
        "      \"id\": x[\"id\"] if \"id\" in x else x[\"example_id\"],\n",
        "      \"label\": label,\n",
        "      \"input_ids\": torch.tensor([cf[\"input_ids\"] for cf in choices_features]),\n",
        "      \"attention_mask\": torch.tensor([cf[\"attention_mask\"] for cf in choices_features]),\n",
        "    }\n",
        "\n",
        "\n",
        "def get_dataloader_race(datadir=None, cachedir: str = \"/content/drive/MyDrive/TFM/RACE_DATASET/race_cache\"):\n",
        "\n",
        "    if datadir is None:\n",
        "      datadir = \"./RACE\"\n",
        "\n",
        "    datadir = Path(datadir)\n",
        "    cachedir = Path(cachedir)\n",
        "    batch_size = 8\n",
        "\n",
        "    tokenizer = MODEL_TKN.from_pretrained(MODEL_ID, do_lower_case=True)\n",
        "    preprocessor = partial(preprocess, tokenizer)\n",
        "\n",
        "    train_samples = []\n",
        "    for grade in (\"middle\", \"high\"):\n",
        "        for _path in (datadir / \"train\" / grade).iterdir():\n",
        "            train_samples.append(json.loads(_path.read_text()))\n",
        "    train = raw_samples_to_dataset(train_samples)\n",
        "    train_dataloader = DataLoader(\n",
        "      train.map(preprocessor).save(cachedir / \"train.cache\"),\n",
        "      sampler=RandomSampler(train),\n",
        "      batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    val_samples = []\n",
        "    for grade in (\"middle\", \"high\"):\n",
        "        for _path in (datadir / \"dev\" / grade).iterdir():\n",
        "            val_samples.append(json.loads(_path.read_text()))\n",
        "\n",
        "    val = raw_samples_to_dataset(val_samples)\n",
        "    val_dataloader = DataLoader(\n",
        "      val.map(preprocessor).save(cachedir / \"val.cache\"),\n",
        "      sampler=SequentialSampler(val),\n",
        "      batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    test_samples = []\n",
        "    for grade in (\"middle\", \"high\"):\n",
        "        for _path in (datadir / \"test\" / grade).iterdir():\n",
        "            test_samples.append(json.loads(_path.read_text()))\n",
        "    test = raw_samples_to_dataset(test_samples)\n",
        "    test_dataloader = DataLoader(\n",
        "      test.map(preprocessor).save(cachedir / \"test.cache\"),\n",
        "      sampler=SequentialSampler(test),\n",
        "      batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    return train_dataloader, val_dataloader, test_dataloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlw4I7o8qnPf"
      },
      "source": [
        "## EE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIVdij-4qW4G"
      },
      "outputs": [],
      "source": [
        "def preprocess_ee(tokenizer: MODEL_TKN, x: Dict) -> Dict:\n",
        "\n",
        "    choices_features = []\n",
        "\n",
        "    option: str\n",
        "    for option in x[\"options\"]:\n",
        "        text_a = x[\"article\"]\n",
        "        text_b = x[\"question\"] + \" \" + option\n",
        "\n",
        "        inputs = tokenizer.encode_plus(\n",
        "                text_a,\n",
        "                text_b,\n",
        "                add_special_tokens=True,\n",
        "                max_length=MAX_LEN\n",
        "                )\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        pad_token_id = tokenizer.pad_token_id\n",
        "        padding_length = MAX_LEN - len(input_ids)\n",
        "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
        "        attention_mask = attention_mask + ([0] * padding_length)\n",
        "\n",
        "        assert len(input_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(input_ids), MAX_LEN)\n",
        "        assert len(attention_mask) == MAX_LEN, \"Error with input length {} vs {}\".format(len(attention_mask), MAX_LEN)\n",
        "\n",
        "        choices_features.append({\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask\n",
        "        })\n",
        "\n",
        "    labels = label_map.get(x[\"answer\"], -1)\n",
        "    label = torch.tensor(labels).long()\n",
        "\n",
        "    return {\n",
        "      \"id\": x[\"id\"],\n",
        "      \"label\": label,\n",
        "      \"input_ids\": torch.tensor([cf[\"input_ids\"] for cf in choices_features]),\n",
        "      \"attention_mask\": torch.tensor([cf[\"attention_mask\"] for cf in choices_features]),\n",
        "    }\n",
        "\n",
        "def raw_samples_to_dataset_ee(samples, size=100):\n",
        "    datas = []\n",
        "    for sample in samples:\n",
        "      if \"answers\" in sample:\n",
        "        for idx in range(len(sample[\"answers\"])):\n",
        "            _id = sample[\"id\"] if \"id\" in sample else sample[\"example_id\"]\n",
        "            _article = sample[\"article\"]\n",
        "            _answer = sample[\"answers\"][idx]\n",
        "            _options = sample[\"options\"][idx]\n",
        "            _question = sample[\"questions\"][idx]\n",
        "\n",
        "            data = {\n",
        "              \"id\": _id,\n",
        "              \"article\": _article,\n",
        "              \"answer\": _answer,\n",
        "              \"options\": _options,\n",
        "              \"question\": _question,\n",
        "            }\n",
        "            datas.append(data)\n",
        "      else:\n",
        "        datas.append(sample) # take into account autogenerated docs\n",
        "            \n",
        "    return datas\n",
        "\n",
        "def get_dataloader_ee(train_val_dir, test_dirs=None, extensions=None, cachedir: str = \"/content/drive/MyDrive/TFM/EntranceExam/ee_cache_en_ext\"):\n",
        "    if test_dirs is None:\n",
        "      test_dirs = [\n",
        "        '/content/drive/MyDrive/TFM/EntranceExam/rc-test-english-2013.json', \n",
        "        '/content/drive/MyDrive/TFM/EntranceExam/rc-test-english-2014.json',\n",
        "      ]\n",
        "\n",
        "    train_val_dir = Path(train_val_dir)\n",
        "    cachedir = Path(cachedir)\n",
        "    batch_size = 8\n",
        "\n",
        "    tokenizer = MODEL_TKN.from_pretrained(MODEL_ID, do_lower_case=True)\n",
        "    preprocessor = partial(preprocess_ee, tokenizer)\n",
        "\n",
        "    train_val_samples = pd.read_json(train_val_dir)['data'].tolist()\n",
        "    test_samples = [pd.read_json(datadir)['data'].tolist() for datadir in test_dirs]\n",
        "    test_samples = [x for xs in test_samples for x in xs]\n",
        "\n",
        "    test_samples = raw_samples_to_dataset_ee(test_samples)\n",
        "    train_val_samples = raw_samples_to_dataset_ee(train_val_samples)\n",
        "\n",
        "    if extensions is not None:\n",
        "      train_val_samples = [*json.load(open(extensions)), *train_val_samples]\n",
        "\n",
        "    div = 4*len(train_val_samples)//5\n",
        "    train = lf.Dataset(train_val_samples[:div])\n",
        "    val = lf.Dataset(train_val_samples[div:])\n",
        "    test = lf.Dataset(test_samples)\n",
        "\n",
        "    print(f\"Configuring trainer with {len(train_val_samples[:div])} samples\")\n",
        "    train_dataloader = DataLoader(\n",
        "      train.map(preprocessor),#.save(cachedir / \"train.cache\"),\n",
        "      sampler=RandomSampler(train),\n",
        "      batch_size=batch_size\n",
        "    )\n",
        "    print(f\"Configuring val with {len(train_val_samples[div:])} samples\")\n",
        "    val_dataloader = DataLoader(\n",
        "      val.map(preprocessor),#.save(cachedir / \"val.cache\"),\n",
        "      sampler=SequentialSampler(val),\n",
        "      batch_size=batch_size\n",
        "    )\n",
        "    print(f\"Configuring test with {len(test_samples)} samples\")\n",
        "    test_dataloader = DataLoader(\n",
        "      test.map(preprocessor),#.save(cachedir / \"test.cache\"),\n",
        "      sampler=SequentialSampler(test),\n",
        "      batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    return train_dataloader, val_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oYh_1j7b2va"
      },
      "source": [
        "## Model Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OJU_92TbyNZ"
      },
      "outputs": [],
      "source": [
        "class Model(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, mode, model_id=MODEL_ID, model_trf=MODEL_TRF, num_labels=NUM_LABELS, **kwargs):\n",
        "        super(Model, self).__init__()\n",
        "        model = model_trf.from_pretrained(model_id, num_labels=num_labels)\n",
        "        self.model = model\n",
        "\n",
        "        dataloader = get_dataloader_race if mode == \"RACE\" else get_dataloader_ee\n",
        "        train_dataloader, val_dataloader, test_dataloader = dataloader(**kwargs)\n",
        "        self._train_dataloader = train_dataloader\n",
        "        self._val_dataloader = val_dataloader\n",
        "        self._test_dataloader = test_dataloader\n",
        "        #self.automatic_optimization=False\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.model.parameters(), lr=0.02)\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        labels = batch[\"label\"]\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        #token_type_ids = batch[\"token_type_ids\"]\n",
        "\n",
        "        loss, _ = self.model(\n",
        "                input_ids,\n",
        "                #token_type_ids=token_type_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "                return_dict=False\n",
        "                )\n",
        "\n",
        "        tqdm_dict = {\"train_loss\": loss}\n",
        "        output = OrderedDict({\n",
        "            \"loss\": loss,\n",
        "            \"progress_bar\": tqdm_dict,\n",
        "            \"log\": tqdm_dict,\n",
        "            })\n",
        "\n",
        "        return output\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        labels = batch[\"label\"]\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        #token_type_ids = batch[\"token_type_ids\"]\n",
        "\n",
        "        loss, logits = self.model(\n",
        "                input_ids,\n",
        "                #token_type_ids=token_type_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "                return_dict=False\n",
        "                )\n",
        "  \n",
        "        labels_hat = torch.argmax(logits, dim=1)\n",
        "        correct_count = torch.sum(labels == labels_hat)\n",
        "        val_acc = correct_count/len(labels)\n",
        "        if self.on_gpu:\n",
        "            correct_count = correct_count.cuda(loss.device.index)\n",
        "        self.log(\"val_loss\", loss)\n",
        "        self.log(\"val_acc\", val_acc)\n",
        "        output = OrderedDict({\n",
        "                \"val_loss\": loss,\n",
        "                \"val_acc\": val_acc,\n",
        "                \"correct_count\": correct_count,\n",
        "                \"batch_size\": len(labels)\n",
        "                })\n",
        "\n",
        "        return output\n",
        "  \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        labels = batch[\"label\"]\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        #token_type_ids = batch[\"token_type_ids\"]\n",
        "\n",
        "        loss, logits = self.model(\n",
        "                input_ids,\n",
        "                #token_type_ids=token_type_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "                return_dict=False\n",
        "                )\n",
        "  \n",
        "        labels_hat = torch.argmax(logits, dim=1)\n",
        "        correct_count = torch.sum(labels == labels_hat)\n",
        "\n",
        "        if self.on_gpu:\n",
        "            correct_count = correct_count.cuda(loss.device.index)\n",
        "\n",
        "        self.log(\"test_acc\", correct_count / len(labels))\n",
        "        \n",
        "    def validation_end(self, outputs):\n",
        "        val_acc = sum([out[\"correct_count\"] for out in outputs]).float() / sum(out[\"batch_size\"] for out in outputs)\n",
        "        val_loss = sum([out[\"val_loss\"] for out in outputs]) / len(outputs)\n",
        "        tqdm_dict = {\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_acc\": val_acc,\n",
        "                }\n",
        "        self.log(\"val_loss\", val_loss)\n",
        "        self.log(\"val_acc\", val_acc)\n",
        "        return {\"progress_bar\": tqdm_dict, \"log\": tqdm_dict, \"val_loss\": val_loss}\n",
        "\n",
        "    # @pl.data_loader\n",
        "    def train_dataloader(self):\n",
        "        return self._train_dataloader\n",
        "\n",
        "    # @pl.data_loader\n",
        "    def val_dataloader(self):\n",
        "        return self._val_dataloader\n",
        "\n",
        "    # @pl.data_loader\n",
        "    def test_dataloader(self):\n",
        "        return self._test_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJlCvU23ujSP"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l5odZFuG78n"
      },
      "source": [
        "### EE - no extensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZRD_rIPdDQM"
      },
      "outputs": [],
      "source": [
        "early_stop_callback = EarlyStopping(\n",
        "  monitor=\"val_acc\",\n",
        "  min_delta=0.0,\n",
        "  patience=4,\n",
        "  verbose=True,\n",
        "  mode=\"max\",\n",
        ")\n",
        "\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "  gpus=1,\n",
        "  callbacks=[early_stop_callback],\n",
        "  #limit_train_batches=0.0001,\n",
        "  #limit_test_batches=0.0001,\n",
        "  #limit_val_batches=0.01,\n",
        "  #max_epochs=5,\n",
        "  # max_time=\"00:00:00:03\", # max time: 1 minute\n",
        "  #max_steps=1,\n",
        "  default_root_dir='/content/drive/MyDrive/TFM/EntrranceExam/ee_checkpoints'\n",
        ")\n",
        "\n",
        "\n",
        "train_val_dir = '/content/drive/MyDrive/TFM/EntranceExam/rc-test-english-2015.json'\n",
        "\n",
        "results = []\n",
        "for i in range(10):\n",
        "  model = Model(\"ee\", train_val_dir=train_val_dir)\n",
        "  fitted = trainer.fit(model)\n",
        "  results.append(trainer.test(model))\n",
        "\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8KZq4H13u5-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.mean([res[0]['test_acc'] for res in results])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4M7SY2jIc8D"
      },
      "source": [
        "### EE - extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGljn0P6fUvh"
      },
      "source": [
        "Experiment zero consists on texts generated artificially from the rc-test-english-2015 dataset + the original dataset. We're going to be generating 190 new answers, so effectively doing a x3 on the available data for training.\n",
        "Let's see where we get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4N74-R9TEUS"
      },
      "outputs": [],
      "source": [
        "early_stop_callback = EarlyStopping(\n",
        "  monitor=\"val_acc\",\n",
        "  min_delta=0.0,\n",
        "  patience=3,\n",
        "  verbose=True,\n",
        "  mode=\"max\",\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "  gpus=1,\n",
        "  callbacks=[early_stop_callback])\n",
        "\n",
        "\n",
        "train_val_dir = '/content/drive/MyDrive/TFM/EntranceExam/rc-test-english-2015.json'\n",
        "extensions='/content/drive/MyDrive/TFM/EntranceExam/ee_cache_en/experiment0.json'\n",
        "\n",
        "\n",
        "results_ext = []\n",
        "for i in range(10):\n",
        "\n",
        "  ext_model = Model(\"ee\", train_val_dir=train_val_dir, extensions=extensions)\n",
        "\n",
        "  fitted = trainer.fit(ext_model)\n",
        "  results_ext.append(trainer.test(ext_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk0hEeM23-AT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.mean([res[0]['test_acc'] for res in results_ext])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EE - larger extensions"
      ],
      "metadata": {
        "id": "6puByoxxprCc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za-lDX0vpp1X"
      },
      "source": [
        "## Model checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQkU1aVIpsa7"
      },
      "outputs": [],
      "source": [
        "loaded_model = Model.load_from_checkpoint(checkpoint_path=\"example.ckpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPlEZbdfqSjK"
      },
      "source": [
        "## Save model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuwFzkf7yTaN"
      },
      "outputs": [],
      "source": [
        "model.training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-kmdxnpv4zM"
      },
      "outputs": [],
      "source": [
        "model.model.save_pretrained(\"/content/drive/MyDrive/Dataset_reviews/baseline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OEEftLA2Fpm"
      },
      "outputs": [],
      "source": [
        "saved_model = MODEL_TRF.from_pretrained(\"baseline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpyL-qJ54Hz3"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "  gpus=1,\n",
        "  #callbacks=[early_stop_callback],\n",
        "  #limit_train_batches=0.0001,\n",
        "  #limit_test_batches=0.00001,\n",
        "  # limit_val_batches=0.0,\n",
        "  #max_epochs=5,\n",
        "  max_time=\"00:00:00:03\", # max time: 1 minute\n",
        "  #max_steps=1\n",
        ")\n",
        "\n",
        "saved_model = Model(\"./RACE\", model_id=\"patata\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6ZgzkWl5Wtm"
      },
      "outputs": [],
      "source": [
        "saved_model.training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1BvQh2D5QHQ"
      },
      "outputs": [],
      "source": [
        "trainer.test(saved_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvIWZbkMEabk"
      },
      "source": [
        "---\n",
        "# 🤗 Transformers\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pip"
      ],
      "metadata": {
        "id": "nhajgU6UjsP_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRCXWj7QGMaE"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "Kz_b0FmAYRGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Union, Dict\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from datasets import load_dataset, Dataset, ClassLabel, DatasetDict\n",
        "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer, AutoTokenizer\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy"
      ],
      "metadata": {
        "id": "6-YpjbEJYXrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataCollator"
      ],
      "metadata": {
        "id": "xOxQkApNYR8Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRgDpyJvk32n"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorForMultipleChoice:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features):\n",
        "        label_name = \"labels\"\n",
        "        labels = [feature.pop(label_name) for feature in features]\n",
        "        batch_size = len(features)\n",
        "        num_choices = len(features[0][\"input_ids\"])\n",
        "        flattened_features = [\n",
        "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
        "        ]\n",
        "        flattened_features = sum(flattened_features, [])\n",
        "\n",
        "        batch = self.tokenizer.pad(\n",
        "            flattened_features,\n",
        "            padding=True,\n",
        "            max_length=512,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
        "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "q59SDTdWZWO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512\n",
        "NUM_LABELS = 4\n",
        "label_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
        "# RACE \n",
        "RACE_DIR = '/content/drive/MyDrive/TFM/RACE_DATASET/output_dir/RACE'"
      ],
      "metadata": {
        "id": "FnAlz0lJLIT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def append_option(question, option):\n",
        "    return question.replace(\"_\", f\" {option}\" ) if \"_\" in question else f\"{question} {option}\"\n",
        "\n",
        "def preprocess(tokenizer, exam):\n",
        "    first_sentences = [exam[\"article\"]] * len(exam[\"options\"])\n",
        "    second_sentences = [append_option(exam[\"question\"], option) for option in exam[\"options\"]]\n",
        "\n",
        "    return tokenizer(first_sentences, second_sentences, truncation=True)\n",
        "\n",
        "def preprocess_dataset(ds, tokenizer):\n",
        "  tokenized_ds = ds.map(partial(preprocess, tokenizer))\n",
        "  tokenized_ds = tokenized_ds.rename_column(\"answer\", \"labels\")\n",
        "  if \"train\" in tokenized_ds:\n",
        "    id_key = \"id\" if \"id\" in tokenized_ds[\"train\"].features else \"example_id\"\n",
        "  else:\n",
        "    id_key = \"id\" if \"id\" in tokenized_ds.features else \"example_id\"\n",
        "\n",
        "  tokenized_ds = tokenized_ds.remove_columns([\"options\", \"question\", \"article\", ])\n",
        "  tokenized_ds = tokenized_ds.cast_column(\"labels\", ClassLabel(num_classes=4, names=[\"A\", \"B\", \"C\", \"D\"]))\n",
        "  return tokenized_ds\n",
        "\n",
        "\n",
        "def peak_encoding(ds, tokenizer):\n",
        "  accepted_keys = [\"input_ids\", \"attention_mask\", \"labels\", \"label\" if \"label\" in  ds[0].keys() else \"labels\"]\n",
        "\n",
        "  features = [{k: v for k, v in ds[i].items() if k in accepted_keys} for i in range(10)]\n",
        "  batch = DataCollatorForMultipleChoice(tokenizer)(features)\n",
        "  return [tokenizer.decode(batch[\"input_ids\"][0][i].tolist()) for i in range(4)]"
      ],
      "metadata": {
        "id": "Ne1gBZHuYlV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def raw_samples_to_dataset_ee(samples):\n",
        "    datas = []\n",
        "    for sample in samples:\n",
        "      if \"answers\" in sample:\n",
        "        for idx in range(len(sample[\"answers\"])):\n",
        "            _id = sample[\"id\"] if \"id\" in sample else sample[\"example_id\"]\n",
        "            _article = sample[\"article\"]\n",
        "            _answer = sample[\"answers\"][idx]\n",
        "            _options = sample[\"options\"][idx]\n",
        "            _question = sample[\"questions\"][idx]\n",
        "\n",
        "            data = {\n",
        "              \"id\": _id,\n",
        "              \"article\": _article,\n",
        "              \"answer\": _answer,\n",
        "              \"options\": _options,\n",
        "              \"question\": _question,\n",
        "            }\n",
        "            datas.append(data)\n",
        "      else:\n",
        "        datas.append(sample) # take into account autogenerated docs\n",
        "            \n",
        "    return datas\n",
        "\n",
        "def load_ee(train_val_dir='/content/drive/MyDrive/TFM/EntranceExam/rc-test-english-2015.json', test_dirs=None, extensions=None, split=True):\n",
        "    if test_dirs is None:\n",
        "      test_dirs = [\n",
        "        '/content/drive/MyDrive/TFM/EntranceExam/rc-test-english-2013.json', \n",
        "        '/content/drive/MyDrive/TFM/EntranceExam/rc-test-english-2014.json',\n",
        "      ]\n",
        "\n",
        "    train_val_dir = Path(train_val_dir)\n",
        "\n",
        "    train_val_samples = pd.read_json(train_val_dir)['data'].tolist()\n",
        "    test_samples = [pd.read_json(datadir)['data'].tolist() for datadir in test_dirs]\n",
        "    test_samples = [x for xs in test_samples for x in xs]\n",
        "\n",
        "    test_samples = raw_samples_to_dataset_ee(test_samples)\n",
        "    train_val_samples = raw_samples_to_dataset_ee(train_val_samples)\n",
        "\n",
        "    if extensions is not None:\n",
        "      train_val_samples = [*json.load(open(extensions)), *train_val_samples]\n",
        "\n",
        "    if not split:\n",
        "      return Dataset.from_pandas(pd.DataFrame([*train_val_samples, *test_samples]))\n",
        "    else:\n",
        "      div = 4*len(train_val_samples)//5\n",
        "      return Dataset.from_pandas(pd.DataFrame(train_val_samples[:div])), Dataset.from_pandas(pd.DataFrame(train_val_samples[div:])), Dataset.from_pandas(pd.DataFrame(test_samples))"
      ],
      "metadata": {
        "id": "TCks3_q3KN5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "c428J8ViZf9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_baseline_acc(model_name, tokenized_ds):\n",
        "  model = AutoModelForMultipleChoice.from_pretrained(model_name)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True, truncation=True, use_fast=True)\n",
        "  training_args = TrainingArguments(\n",
        "    output_dir=\"./baseline\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "  )\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      tokenizer=tokenizer,\n",
        "      data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
        "      compute_metrics=compute_metrics\n",
        "  )\n",
        "  output = trainer.predict(tokenized_ds)\n",
        "  print(f\"Baseline accuracy for {model_name}\")\n",
        "  print(acc(tokenized_ds[\"labels\"], [np.argmax(x) for x in output.predictions]))\n",
        "  #return output\n",
        "\n",
        "def compute_metrics(eval_predictions):\n",
        "    predictions, label_ids = eval_predictions\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}\n",
        "\n",
        "def acc(labels, predictions):\n",
        "  return sum([l==p for l,p in zip(labels, predictions)]) / len(predictions)\n"
      ],
      "metadata": {
        "id": "uUHrtDKIpwtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 🔮 Model baselines\n",
        "---"
      ],
      "metadata": {
        "id": "o8GSXiF7c0tC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distilbert base"
      ],
      "metadata": {
        "id": "kiWHdTo9aQc4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYzDQ7AfEcrv"
      },
      "outputs": [],
      "source": [
        "race = load_dataset(\"race\", \"all\")\n",
        "ee = load_ee(split=False)\n",
        "\n",
        "tokenizer_distilbert = AutoTokenizer.from_pretrained(\"LIAMF-USP/roberta-large-finetuned-race\", use_fast=True, do_lower_case=True, truncation=True)\n",
        "preprocess_dataset(ee, tokenizer_distilbert)\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True, truncation=True, use_fast=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_race = preprocess_dataset(race, tokenizer)\n",
        "tokenized_ee = preprocess_dataset(ee, tokenizer)"
      ],
      "metadata": {
        "id": "uCC-4bS4lbe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_baseline_acc('distilbert-base-uncased', tokenized_ee)"
      ],
      "metadata": {
        "id": "_U1k7bfEY89i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_baseline_acc('distilbert-base-uncased', tokenized_race[\"test\"])"
      ],
      "metadata": {
        "id": "MXewGMPGk6xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## bert-large-uncased"
      ],
      "metadata": {
        "id": "308DZnmhIXOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache() "
      ],
      "metadata": {
        "id": "QSt5UfpZ8D4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_bertlarge = AutoTokenizer.from_pretrained(\n",
        "\"bert-large-uncased\", use_fast=True)\n",
        "tokenized_eelarge = preprocess_dataset(ee, tokenizer_bertlarge)\n",
        "get_baseline_acc('bert-large-uncased', tokenized_eelarge)"
      ],
      "metadata": {
        "id": "3AVKXt13q0dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## roberta-large"
      ],
      "metadata": {
        "id": "7dGbbeqnIZl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /root/.cache/huggingface"
      ],
      "metadata": {
        "id": "OEB1s4gxv7pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_roberta = AutoTokenizer.from_pretrained(\n",
        "\"roberta-large\", use_fast=True)\n",
        "tokenized_ee_roberta = preprocess_dataset(ee, tokenizer_roberta)\n",
        "get_baseline_acc('roberta-large', tokenized_ee_roberta)"
      ],
      "metadata": {
        "id": "BEc7W33g17Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## roberta-large finetuned race"
      ],
      "metadata": {
        "id": "3Jgzjj3shdSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ee_roberta = preprocess_dataset(ee, tokenizer_roberta)\n",
        "\n",
        "get_baseline_acc('LIAMF-USP/roberta-large-finetuned-race', tokenized_ee_roberta)"
      ],
      "metadata": {
        "id": "ZQqBmEcPso2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.mem_get_info() "
      ],
      "metadata": {
        "id": "t5fb2m17CMUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ee_train, ee_val, ee_test = load_ee(extensions='/content/drive/MyDrive/TFM/EntranceExam/ee_cache_en/experiment1.json', split=True)\n",
        "tokenizer_distilbert = AutoTokenizer.from_pretrained(\"LIAMF-USP/roberta-large-finetuned-race\", use_fast=True, do_lower_case=True, truncation=True)\n",
        "\n",
        "ee_train = preprocess_dataset(ee_train, tokenizer_distilbert)\n",
        "ee_val = preprocess_dataset(ee_val, tokenizer_distilbert)\n",
        "ee_test = preprocess_dataset(ee_test, tokenizer_distilbert)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=1,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=AutoModelForMultipleChoice.from_pretrained('LIAMF-USP/roberta-large-finetuned-race'),\n",
        "    args=training_args,\n",
        "    train_dataset=ee_train,\n",
        "    eval_dataset=ee_val,\n",
        "    tokenizer=tokenizer_distilbert,\n",
        "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer_distilbert),\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "output = trainer.predict(ee_test)\n",
        "acc(ee_test[\"labels\"], [np.argmax(x) for x in output.predictions])"
      ],
      "metadata": {
        "id": "Wg8n2AgI44dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 🧪   Experiments\n",
        "---"
      ],
      "metadata": {
        "id": "-prCFZ1gevvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📈 EntranceExams\n"
      ],
      "metadata": {
        "id": "bC5vUo8f2Cnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline- no training"
      ],
      "metadata": {
        "id": "yVHx6iCVfWJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(**{\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"fp16\": True,\n",
        "    \"fp16_opt_level\": \"O1\",\n",
        "    \"save_total_limit\": 0,\n",
        "    \"save_steps\": 0,\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"num_train_epochs\": 3,\n",
        "    \"per_device_eval_batch_size\": 8,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"learning_rate\": 5.0e-05,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"output_dir\" : \"./results5\",\n",
        "    \"eval_steps\": 10\n",
        "  })\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=AutoModelForMultipleChoice.from_pretrained('bert-base-uncased'),\n",
        "    args=training_args,\n",
        "    train_dataset=ee_train,\n",
        "    eval_dataset=ee_val,\n",
        "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer_bert),\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "output_notrain = trainer.predict(ee_test)\n",
        "acc(ee_test[\"labels\"], [np.argmax(x) for x in output_notrain.predictions])"
      ],
      "metadata": {
        "id": "c9RD9BI2fZDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base case - training so little it has no effect"
      ],
      "metadata": {
        "id": "hsMt3STxXWJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ee_train, ee_val, ee_test = load_ee(split=True)\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True, do_lower_case=True, truncation=True)\n",
        "\n",
        "ee_train = preprocess_dataset(ee_train, tokenizer_bert)\n",
        "ee_val = preprocess_dataset(ee_val, tokenizer_bert)\n",
        "ee_test = preprocess_dataset(ee_test, tokenizer_bert)"
      ],
      "metadata": {
        "id": "f_LBY224ar_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(**{\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"fp16\": True,\n",
        "    \"fp16_opt_level\": \"O1\",\n",
        "    \"save_total_limit\": 0,\n",
        "    \"save_steps\": 0,\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"num_train_epochs\": 3,\n",
        "    \"per_device_eval_batch_size\": 8,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"learning_rate\": 5.0e-05,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"output_dir\" : \"./results5\",\n",
        "    \"eval_steps\": 10\n",
        "  })\n",
        "\n",
        "outputs = []\n",
        "for i in range(10):\n",
        "  trainer = Trainer(\n",
        "      model=AutoModelForMultipleChoice.from_pretrained('bert-base-uncased'),\n",
        "      args=training_args,\n",
        "      train_dataset=ee_train,\n",
        "      eval_dataset=ee_val,\n",
        "      data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer_bert),\n",
        "      compute_metrics=compute_metrics\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  output_ee = trainer.predict(ee_test)\n",
        "  outputs.append(acc(ee_test[\"labels\"], [np.argmax(x) for x in output_ee.predictions]))\n",
        "\n",
        "print(f\"Mean of outputs is {np.mean(outputs)}\")"
      ],
      "metadata": {
        "id": "3mzfXvNcULEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "id": "Cj4l4vwLcskL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With Extension -1st experiment - worse"
      ],
      "metadata": {
        "id": "IN0frHDuUJBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ee_train, ee_val, ee_test = load_ee(extensions='/content/drive/MyDrive/TFM/EntranceExam/ee_cache_en/experiment0.json', split=True)\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True, do_lower_case=True, truncation=True)\n",
        "\n",
        "ee_train = preprocess_dataset(ee_train, tokenizer_bert)\n",
        "ee_val = preprocess_dataset(ee_val, tokenizer_bert)\n",
        "ee_test = preprocess_dataset(ee_test, tokenizer_bert)\n",
        "\n",
        "training_args = TrainingArguments(**{\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"fp16\": True,\n",
        "    \"fp16_opt_level\": \"O1\",\n",
        "    \"save_total_limit\": 0,\n",
        "    \"save_steps\": 0,\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"num_train_epochs\": 3,\n",
        "    \"per_device_eval_batch_size\": 8,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"learning_rate\": 5.0e-05,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"output_dir\" : \"./results5\",\n",
        "    \"eval_steps\": 10\n",
        "  })\n",
        "\n",
        "outputs_ext = []\n",
        "for i in range(10):\n",
        "  trainer = Trainer(\n",
        "    model=AutoModelForMultipleChoice.from_pretrained('bert-base-uncased'),\n",
        "    args=training_args,\n",
        "    train_dataset=ee_train,\n",
        "    eval_dataset=ee_val,\n",
        "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer_distilbert),\n",
        "    compute_metrics=compute_metrics\n",
        "  )\n",
        "  trainer.train()\n",
        "  output_ee_ext = trainer.predict(ee_test)\n",
        "  outputs_ext.append(acc(ee_test[\"labels\"], [np.argmax(x) for x in output_ee_ext.predictions]))\n",
        "\n",
        "print(f\"Mean of outputs is {np.mean(outputs_ext)}\")"
      ],
      "metadata": {
        "id": "8Wc_vYu-TMA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With Extension - 2nd experiment - even worse"
      ],
      "metadata": {
        "id": "HTx3MYVDhcVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ee_train, ee_val, ee_test = load_ee(extensions='/content/drive/MyDrive/TFM/EntranceExam/ee_cache_en/experiment1.json', split=True)\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True, do_lower_case=True, truncation=True)\n",
        "\n",
        "ee_train = preprocess_dataset(ee_train, tokenizer_bert)\n",
        "ee_val = preprocess_dataset(ee_val, tokenizer_bert)\n",
        "ee_test = preprocess_dataset(ee_test, tokenizer_bert)"
      ],
      "metadata": {
        "id": "uRm7LElWhgic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(**{\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"fp16\": True,\n",
        "    \"fp16_opt_level\": \"O1\",\n",
        "    \"save_total_limit\": 0,\n",
        "    \"save_steps\": 0,\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"num_train_epochs\": 3,\n",
        "    \"per_device_eval_batch_size\": 8,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"learning_rate\": 5.0e-05,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"output_dir\" : \"./results5\",\n",
        "    \"eval_steps\": 1000\n",
        "  })\n",
        "\n",
        "trainer = Trainer(\n",
        "  model=AutoModelForMultipleChoice.from_pretrained('bert-base-uncased'),\n",
        "  args=training_args,\n",
        "  train_dataset=ee_train,\n",
        "  eval_dataset=ee_val,\n",
        "  data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer_bert),\n",
        "  compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()\n",
        "output_ee_ext = trainer.predict(ee_test)\n",
        "acc(ee_test[\"labels\"], [np.argmax(x) for x in output_ee_ext.predictions])"
      ],
      "metadata": {
        "id": "UbUIWZ_chbJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With Extensions : sentences"
      ],
      "metadata": {
        "id": "zGy3G1UwZKFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ee_train, ee_val, ee_test = load_ee(extensions='/content/drive/MyDrive/TFM/EntranceExam/ee_cache_en/experiment2-sent.json', split=True)\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True, do_lower_case=True, truncation=True)\n",
        "\n",
        "ee_train = preprocess_dataset(ee_train, tokenizer_bert)\n",
        "ee_val = preprocess_dataset(ee_val, tokenizer_bert)\n",
        "ee_test = preprocess_dataset(ee_test, tokenizer_bert)"
      ],
      "metadata": {
        "id": "W1Az_nnYZMnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(**{\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"fp16\": True,\n",
        "    \"fp16_opt_level\": \"O1\",\n",
        "    \"save_total_limit\": 0,\n",
        "    \"save_steps\": 0,\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"num_train_epochs\": 3,\n",
        "    \"per_device_eval_batch_size\": 8,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"learning_rate\": 5.0e-05,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"output_dir\" : \"./results5\",\n",
        "    \"eval_steps\": 1000\n",
        "  })\n",
        "\n",
        "trainer = Trainer(\n",
        "  model=AutoModelForMultipleChoice.from_pretrained('bert-base-uncased'),\n",
        "  args=training_args,\n",
        "  train_dataset=ee_train,\n",
        "  eval_dataset=ee_val,\n",
        "  data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer_bert),\n",
        "  compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()\n",
        "output_ee_ext = trainer.predict(ee_test)\n",
        "acc(ee_test[\"labels\"], [np.argmax(x) for x in output_ee_ext.predictions])"
      ],
      "metadata": {
        "id": "yAC6Bt_WZOo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🏃 RACE"
      ],
      "metadata": {
        "id": "V5SqgSaR3Qse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Race - Bert Base 5k (all)"
      ],
      "metadata": {
        "id": "KQX0BJf0afKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(**{\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"fp16\": True,\n",
        "    \"fp16_opt_level\": \"O1\",\n",
        "    \"save_total_limit\": 0,\n",
        "    \"save_steps\": 0,\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"num_train_epochs\": 3,\n",
        "    \"per_device_eval_batch_size\": 8,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"learning_rate\": 5.0e-05,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"output_dir\" : \"./results5\",\n",
        "    \"eval_steps\": 100\n",
        "  })\n",
        "\n",
        "\n",
        "race = load_dataset(\"race\", \"all\")\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True, do_lower_case=True, truncation=True)\n",
        "tokenized_race = preprocess_dataset(race, tokenizer_bert)\n",
        "\n",
        "bert_base_5k = Trainer(\n",
        "    model=AutoModelForMultipleChoice.from_pretrained('bert-base-uncased'),\n",
        "    args=training_args,\n",
        "    train_dataset=Dataset.from_pandas(Dataset.to_pandas(tokenized_race[\"train\"]).head(5000)),\n",
        "    eval_dataset=Dataset.from_pandas(Dataset.to_pandas(tokenized_race[\"validation\"]).head(1000)),\n",
        "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer_bert),\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "bert_base_5k.train()\n",
        "output = bert_base_5k.predict(tokenized_race[\"test\"])\n",
        "acc(tokenized_race[\"test\"][\"labels\"], [np.argmax(x) for x in output.predictions])"
      ],
      "metadata": {
        "id": "kDjrDszUaucl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RACE- 5k middle"
      ],
      "metadata": {
        "id": "ry5Cmcugia9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(**{\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"fp16\": True,\n",
        "    \"fp16_opt_level\": \"O1\",\n",
        "    \"save_total_limit\": 0,\n",
        "    \"save_steps\": 0,\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"num_train_epochs\": 3,\n",
        "    \"per_device_eval_batch_size\": 8,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"learning_rate\": 5.0e-05,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"output_dir\" : \"/content/drive/MyDrive/TFM/models/results5_middle\",\n",
        "    \"eval_steps\": 100\n",
        "  })\n",
        "\n",
        "\n",
        "race = load_dataset(\"race\", \"middle\")\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True, do_lower_case=True, truncation=True)\n",
        "tokenized_race = preprocess_dataset(race, tokenizer_bert)\n",
        "\n",
        "bert_base_5k_middle = Trainer(\n",
        "    model=AutoModelForMultipleChoice.from_pretrained('bert-base-uncased'),\n",
        "    args=training_args,\n",
        "    train_dataset=Dataset.from_pandas(Dataset.to_pandas(tokenized_race[\"train\"]).head(5000)),\n",
        "    eval_dataset=Dataset.from_pandas(Dataset.to_pandas(tokenized_race[\"validation\"]).head(1000)),\n",
        "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer_bert),\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "bert_base_5k_middle.train()"
      ],
      "metadata": {
        "id": "4gNH3OpYq-Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = bert_base_5k_middle.predict(tokenized_race[\"test\"])\n",
        "acc(tokenized_race[\"test\"][\"labels\"], [np.argmax(x) for x in output.predictions])"
      ],
      "metadata": {
        "id": "BnzjbDUOyUil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RACE - 5k middle + 5k synthetic from those 5k (sent) .56"
      ],
      "metadata": {
        "id": "jPolQxymicz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extension_sents = pd.read_json(open('/content/drive/MyDrive/TFM/RACE_DATASET/race_extensions/train_15k_sent.json'))\n",
        "extension_sents[\"example_id\"] = extension_sents[\"id\"]\n",
        "del extension_sents[\"id\"]\n",
        "\n",
        "race = load_dataset(\"race\", \"middle\")\n",
        "\n",
        "df_joined = pd.concat([race[\"train\"].to_pandas().head(5000), extension_sents])"
      ],
      "metadata": {
        "id": "omzQzNc8y9Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity check : ids are the same"
      ],
      "metadata": {
        "id": "Z5IOrpBF2SFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_joined[\"example_id\"].apply(lambda x: x.split(\"middle\")[1]).unique()) == len(race[\"train\"].to_pandas().head(5000)[\"example_id\"].apply(lambda x: x.split(\"middle\")[1]).unique())"
      ],
      "metadata": {
        "id": "lCdsZKSz180f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_joined.shape"
      ],
      "metadata": {
        "id": "hbmoB0qq2XT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True, do_lower_case=True, truncation=True)\n",
        "tokenized_race_5k_5ksent = preprocess_dataset(Dataset.from_pandas(df_joined), tokenizer_bert)"
      ],
      "metadata": {
        "id": "q6FeAtW50Z-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note: we validate **only against real data** (from another split, intersection with taining data is void)"
      ],
      "metadata": {
        "id": "yNkw0zDX-MTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(**{\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"fp16\": True,\n",
        "    \"fp16_opt_level\": \"O1\",\n",
        "    \"save_total_limit\": 0,\n",
        "    \"save_steps\": 0,\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"num_train_epochs\": 2,\n",
        "    \"per_device_eval_batch_size\": 8,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"learning_rate\": 5.0e-05,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"output_dir\" : \"/content/drive/MyDrive/TFM/models/results5k_5ksynth\",\n",
        "    \"eval_steps\": 100\n",
        "  })\n",
        "\n",
        "\n",
        "race = load_dataset(\"race\", \"middle\")\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True, do_lower_case=True, truncation=True)\n",
        "tokenized_race = preprocess_dataset(race, tokenizer_bert)\n",
        "\n",
        "bert_base_5k_5ksent = Trainer(\n",
        "    model=AutoModelForMultipleChoice.from_pretrained('bert-base-uncased'),\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_race_5k_5ksent,\n",
        "    eval_dataset=Dataset.from_pandas(Dataset.to_pandas(tokenized_race[\"validation\"]).head(1000)),\n",
        "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer_bert),\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "bert_base_5k_5ksent.train()"
      ],
      "metadata": {
        "id": "kryACXUX3WN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = bert_base_5k_5ksent.predict(tokenized_race[\"test\"])\n",
        "acc(tokenized_race[\"test\"][\"labels\"], [np.argmax(x) for x in output.predictions])"
      ],
      "metadata": {
        "id": "oPqyNcE53-fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RACE - 5k middle + 5k synthetic from those 5k (words)"
      ],
      "metadata": {
        "id": "3VrHkOAzyHBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extension_words = pd.read_json(open('/content/drive/MyDrive/TFM/RACE_DATASET/race_extensions/train_5k_words.json'))\n",
        "extension_words[\"example_id\"] = extension_words[\"id\"]\n",
        "del extension_words[\"id\"]\n",
        "\n",
        "race = load_dataset(\"race\", \"middle\")\n",
        "\n",
        "df_words = pd.concat([race[\"train\"].to_pandas().head(5000), extension_words])\n",
        "len(df_words[\"example_id\"].apply(lambda x: x.split(\"middle\")[1]).unique()) == len(race[\"train\"].to_pandas().head(5000)[\"example_id\"].apply(lambda x: x.split(\"middle\")[1]).unique())"
      ],
      "metadata": {
        "id": "arsXna_oaDCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_words.shape"
      ],
      "metadata": {
        "id": "J0ozcJXpaVcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True, do_lower_case=True, truncation=True)\n",
        "tokenized_race_5k_5kwords = preprocess_dataset(Dataset.from_pandas(df_words), tokenizer_bert)"
      ],
      "metadata": {
        "id": "YxTrE9Flaec_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(**{\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"fp16\": True,\n",
        "    \"fp16_opt_level\": \"O1\",\n",
        "    \"save_total_limit\": 0,\n",
        "    \"save_steps\": 0,\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"num_train_epochs\": 2,\n",
        "    \"per_device_eval_batch_size\": 8,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"learning_rate\": 5.0e-05,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"output_dir\" : \"/content/drive/MyDrive/TFM/models/results5k_5ksynth\",\n",
        "    \"eval_steps\": 100\n",
        "  })\n",
        "\n",
        "\n",
        "race = load_dataset(\"race\", \"middle\")\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True, do_lower_case=True, truncation=True)\n",
        "tokenized_race = preprocess_dataset(race, tokenizer_bert)\n",
        "\n",
        "bert_base_5k_5kwords = Trainer(\n",
        "    model=AutoModelForMultipleChoice.from_pretrained('bert-base-uncased'),\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_race_5k_5kwords,\n",
        "    eval_dataset=Dataset.from_pandas(Dataset.to_pandas(tokenized_race[\"validation\"]).head(1000)),\n",
        "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer_bert),\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "bert_base_5k_5kwords.train()"
      ],
      "metadata": {
        "id": "dQNElbq0ak1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = bert_base_5k_5kwords.predict(tokenized_race[\"test\"])\n",
        "acc(tokenized_race[\"test\"][\"labels\"], [np.argmax(x) for x in output.predictions])"
      ],
      "metadata": {
        "id": "bl30toNabT-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RACE - 10k middle"
      ],
      "metadata": {
        "id": "JmBE-M3Fik3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(**{\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"fp16\": True,\n",
        "    \"fp16_opt_level\": \"O1\",\n",
        "    \"save_total_limit\": 0,\n",
        "    \"save_steps\": 0,\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"num_train_epochs\": 2,\n",
        "    \"per_device_eval_batch_size\": 8,\n",
        "    \"per_device_train_batch_size\": 2,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"learning_rate\": 5.0e-05,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"output_dir\" : \"/content/drive/MyDrive/TFM/models/results10_middle\",\n",
        "    \"eval_steps\": 250\n",
        "  })\n",
        "\n",
        "\n",
        "race = load_dataset(\"race\", \"middle\")\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True, do_lower_case=True, truncation=True)\n",
        "tokenized_race = preprocess_dataset(race, tokenizer_bert)\n",
        "\n",
        "bert_base_10k_middle = Trainer(\n",
        "    model=AutoModelForMultipleChoice.from_pretrained('bert-base-uncased'),\n",
        "    args=training_args,\n",
        "    train_dataset=Dataset.from_pandas(Dataset.to_pandas(tokenized_race[\"train\"]).head(10000)),\n",
        "    eval_dataset=Dataset.from_pandas(Dataset.to_pandas(tokenized_race[\"validation\"]).head(1000)),\n",
        "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer_bert),\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "bert_base_10k_middle.train()"
      ],
      "metadata": {
        "id": "FvSYLZWZE_Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = bert_base_10k_middle.predict(tokenized_race[\"test\"])\n",
        "acc(tokenized_race[\"test\"][\"labels\"], [np.argmax(x) for x in output.predictions])"
      ],
      "metadata": {
        "id": "_ZJdaHz9FMy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# ✅ Synthetic Datasets Evaluation\n",
        "---"
      ],
      "metadata": {
        "id": "B-9Yxty37PeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do they make sense? We can infer that they're at least somewhat predictable"
      ],
      "metadata": {
        "id": "6PF1ei0jNmFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EE original"
      ],
      "metadata": {
        "id": "6bChNhA6Nqof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ee = load_ee(split=False)\n",
        "ee = preprocess_dataset(ee, tokenizer_bert)\n",
        "outputee = bert_base_5k.predict(ee)\n",
        "acc(ee[\"labels\"], [np.argmax(x) for x in outputee.predictions])"
      ],
      "metadata": {
        "id": "vCS5GaLnNr5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ee = load_ee(split=False)\n",
        "ee = preprocess_dataset(ee, tokenizer_bert)\n",
        "outputee = bert_base_5k_5ksent.predict(ee)\n",
        "acc(ee[\"labels\"], [np.argmax(x) for x in outputee.predictions])"
      ],
      "metadata": {
        "id": "ptwm9kuouri8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EE 100"
      ],
      "metadata": {
        "id": "83GuxSIy9WH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extensions='/content/drive/MyDrive/TFM/EntranceExam/ee_cache_en/experiment0.json'\n",
        "trainy = Dataset.from_pandas(pd.read_json(extensions))\n",
        "trainy = preprocess_dataset(trainy, tokenizer_bert)\n",
        "output2 = bert_base_5k_middle.predict(trainy)\n",
        "acc(trainy[\"labels\"], [np.argmax(x) for x in output2.predictions])"
      ],
      "metadata": {
        "id": "pnrmTiKc7Vng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EE 1000"
      ],
      "metadata": {
        "id": "E6qcfgAd9X5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extensions='/content/drive/MyDrive/TFM/EntranceExam/ee_cache_en/experiment1.json'\n",
        "trainy = Dataset.from_pandas(pd.read_json(extensions))\n",
        "trainy = preprocess_dataset(trainy, tokenizer_bert)\n",
        "output3 = bert_base_5k_middle.predict(trainy)\n",
        "acc(trainy[\"labels\"], [np.argmax(x) for x in output3.predictions])"
      ],
      "metadata": {
        "id": "8P72m4kdLwQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EE 100 Sentences"
      ],
      "metadata": {
        "id": "iriUiwtHNwMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extensions='/content/drive/MyDrive/TFM/EntranceExam/ee_cache_en/experiment2-sent.json'\n",
        "trainy = Dataset.from_pandas(pd.read_json(extensions))\n",
        "trainy = preprocess_dataset(trainy, tokenizer_bert)\n",
        "output3 = bert_base_5k_middle.predict(trainy)\n",
        "acc(trainy[\"labels\"], [np.argmax(x) for x in output3.predictions])"
      ],
      "metadata": {
        "id": "ajtKbi8_Pnsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j57pE3Qpcgur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RACE-high - words - 3k vs RACE-middle 5k"
      ],
      "metadata": {
        "id": "4q27STKkfMOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import walk\n",
        "\n",
        "mypath = '/content/drive/MyDrive/TFM/RACE_DATASET/race_extensions/first_poc/high/'\n",
        "\n",
        "\n",
        "f = []\n",
        "for (dirpath, dirnames, filenames) in walk(mypath):\n",
        "    f.extend(filenames)"
      ],
      "metadata": {
        "id": "TIpTOjh1fb4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extensions = Dataset.from_pandas(pd.DataFrame([json.load(open(mypath + fl)) for fl in f]))\n",
        "trainy = preprocess_dataset(extensions, tokenizer_bert)\n",
        "output_highhigh = bert_base_5k.predict(trainy)\n",
        "acc(trainy[\"labels\"], [np.argmax(x) for x in output_highhigh.predictions])"
      ],
      "metadata": {
        "id": "Vzcv3ejngN03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training over Synthetic RACE"
      ],
      "metadata": {
        "id": "95zwbvrsnpGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It has less preditive power, but still better than baseline."
      ],
      "metadata": {
        "id": "Kkg1p0OpqzIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(**{\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"fp16\": True,\n",
        "    \"fp16_opt_level\": \"O1\",\n",
        "    \"save_total_limit\": 0,\n",
        "    \"save_steps\": 0,\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"num_train_epochs\": 3,\n",
        "    \"per_device_eval_batch_size\": 8,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"learning_rate\": 5.0e-05,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"output_dir\" : \"./results5\",\n",
        "    \"eval_steps\": 100\n",
        "  })\n",
        "\n",
        "\n",
        "bert_base_3k_synth = Trainer(\n",
        "    model=AutoModelForMultipleChoice.from_pretrained('bert-base-uncased'),\n",
        "    args=training_args,\n",
        "    train_dataset=trainy,\n",
        "    eval_dataset=Dataset.from_pandas(Dataset.to_pandas(tokenized_race[\"validation\"]).tail(620)),\n",
        "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer_bert),\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "bert_base_3k_synth.train()\n",
        "\n",
        "output = bert_base_3k_synth.predict(tokenized_race[\"test\"])\n",
        "acc(tokenized_race[\"test\"][\"labels\"], [np.argmax(x) for x in output.predictions])"
      ],
      "metadata": {
        "id": "EYab4EJ8jqd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does this model predict over ee?"
      ],
      "metadata": {
        "id": "rx7KVEjerQ3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ee = load_ee(split=False)\n",
        "ee = preprocess_dataset(ee, tokenizer_bert)\n",
        "output_eesynth = bert_base_3k_synth.predict(ee)\n",
        "acc(ee[\"labels\"], [np.argmax(x) for x in output_eesynth.predictions])"
      ],
      "metadata": {
        "id": "CxnukILqrSj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Worse than baseline, but better than the ee synthetic ones."
      ],
      "metadata": {
        "id": "rUuiiZ-qrlbZ"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5GBZpEenkTeu",
        "Cvqi8105rFuS",
        "H_FI8ftjcrWP",
        "GsfZCmfIKJkl",
        "eM5Bm_md_b51",
        "wE_2lDGY2eSx",
        "ZvIWZbkMEabk",
        "nhajgU6UjsP_",
        "Kz_b0FmAYRGT",
        "xOxQkApNYR8Y",
        "q59SDTdWZWO7",
        "o8GSXiF7c0tC",
        "308DZnmhIXOw",
        "7dGbbeqnIZl0",
        "yVHx6iCVfWJW",
        "hsMt3STxXWJZ",
        "IN0frHDuUJBK",
        "HTx3MYVDhcVW",
        "zGy3G1UwZKFH",
        "V5SqgSaR3Qse",
        "KQX0BJf0afKt",
        "ry5Cmcugia9e",
        "jPolQxymicz0",
        "3VrHkOAzyHBV",
        "JmBE-M3Fik3L",
        "B-9Yxty37PeX",
        "83GuxSIy9WH9",
        "E6qcfgAd9X5D",
        "iriUiwtHNwMh",
        "4q27STKkfMOU",
        "95zwbvrsnpGW"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}